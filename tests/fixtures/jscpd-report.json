{
  "duplicates": [
    {
      "format": "python",
      "lines": 33,
      "fragment": "data = []\n    story = []\n    for line in lines:\n        line = line.decode('utf-8').strip()\n        nid, line = line.split(' ', 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if '\\t' in line:\n            q, a, supporting = line.split('\\t')\n            q = tokenize(q)\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n            data.append((substory, q, a))\n            story.append('')\n        else:\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting=False, max_length=None):\n    '''Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    '''",
      "tokens": 0,
      "firstFile": {
        "name": "examples/babi_rnn.py",
        "start": 91,
        "end": 123,
        "startLoc": {
          "line": 91,
          "column": 5
        },
        "endLoc": {
          "line": 123,
          "column": 8
        }
      },
      "secondFile": {
        "name": "examples/babi_memnn.py",
        "start": 46,
        "end": 79,
        "startLoc": {
          "line": 46,
          "column": 5
        },
        "endLoc": {
          "line": 79,
          "column": 8
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": "data = parse_stories(f.readlines(), only_supporting=only_supporting)\n    flatten = lambda data: reduce(lambda x, y: x + y, data)\n    data = [(flatten(story), q, answer) for story, q, answer in data\n            if not max_length or len(flatten(story)) < max_length]\n    return data\n\n\ndef vectorize_stories(data,",
      "tokens": 0,
      "firstFile": {
        "name": "examples/babi_rnn.py",
        "start": 124,
        "end": 131,
        "startLoc": {
          "line": 124,
          "column": 5
        },
        "endLoc": {
          "line": 131,
          "column": 28
        }
      },
      "secondFile": {
        "name": "examples/babi_memnn.py",
        "start": 80,
        "end": 87,
        "startLoc": {
          "line": 80,
          "column": 5
        },
        "endLoc": {
          "line": 87,
          "column": 28
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": "x = AveragePooling2D(pool_size=8)(x)\n    y = Flatten()(x)\n    outputs = Dense(num_classes,\n                    activation='softmax',\n                    kernel_initializer='he_normal')(y)\n\n    # Instantiate model.\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n\nif",
      "tokens": 0,
      "firstFile": {
        "name": "examples/cifar10_resnet.py",
        "start": 344,
        "end": 355,
        "startLoc": {
          "line": 344,
          "column": 5
        },
        "endLoc": {
          "line": 355,
          "column": 3
        }
      },
      "secondFile": {
        "name": "examples/cifar10_resnet.py",
        "start": 248,
        "end": 259,
        "startLoc": {
          "line": 248,
          "column": 5
        },
        "endLoc": {
          "line": 259,
          "column": 4
        }
      }
    },
    {
      "format": "python",
      "lines": 13,
      "fragment": "),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding='same', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(Conv3D",
      "tokens": 0,
      "firstFile": {
        "name": "examples/conv_lstm.py",
        "start": 24,
        "end": 36,
        "startLoc": {
          "line": 24,
          "column": 49
        },
        "endLoc": {
          "line": 36,
          "column": 15
        }
      },
      "secondFile": {
        "name": "examples/conv_lstm.py",
        "start": 20,
        "end": 28,
        "startLoc": {
          "line": 20,
          "column": 48
        },
        "endLoc": {
          "line": 28,
          "column": 19
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": "print('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint",
      "tokens": 0,
      "firstFile": {
        "name": "examples/imdb_cnn.py",
        "start": 27,
        "end": 38,
        "startLoc": {
          "line": 27,
          "column": 1
        },
        "endLoc": {
          "line": 38,
          "column": 6
        }
      },
      "secondFile": {
        "name": "examples/imdb_bidirectional_lstm.py",
        "start": 23,
        "end": 33,
        "startLoc": {
          "line": 23,
          "column": 1
        },
        "endLoc": {
          "line": 33,
          "column": 8
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": "print('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\n\nmodel = Sequential()\nmodel",
      "tokens": 0,
      "firstFile": {
        "name": "examples/imdb_cnn_lstm.py",
        "start": 39,
        "end": 53,
        "startLoc": {
          "line": 39,
          "column": 1
        },
        "endLoc": {
          "line": 53,
          "column": 6
        }
      },
      "secondFile": {
        "name": "examples/imdb_bidirectional_lstm.py",
        "start": 23,
        "end": 41,
        "startLoc": {
          "line": 23,
          "column": 1
        },
        "endLoc": {
          "line": 41,
          "column": 60
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ")\n    print('Average train sequence length: {}'.format(\n        np.mean(list(map(len, x_train)), dtype=int)))\n    print('Average test sequence length: {}'.format(\n        np.mean(list(map(len, x_test)), dtype=int)))\n\nprint",
      "tokens": 0,
      "firstFile": {
        "name": "examples/imdb_fasttext.py",
        "start": 109,
        "end": 115,
        "startLoc": {
          "line": 109,
          "column": 57
        },
        "endLoc": {
          "line": 115,
          "column": 6
        }
      },
      "secondFile": {
        "name": "examples/imdb_fasttext.py",
        "start": 82,
        "end": 88,
        "startLoc": {
          "line": 82,
          "column": 36
        },
        "endLoc": {
          "line": 88,
          "column": 3
        }
      }
    },
    {
      "format": "python",
      "lines": 18,
      "fragment": ")\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings",
      "tokens": 0,
      "firstFile": {
        "name": "examples/imdb_fasttext.py",
        "start": 113,
        "end": 130,
        "startLoc": {
          "line": 113,
          "column": 52
        },
        "endLoc": {
          "line": 130,
          "column": 69
        }
      },
      "secondFile": {
        "name": "examples/imdb_cnn.py",
        "start": 30,
        "end": 46,
        "startLoc": {
          "line": 30,
          "column": 36
        },
        "endLoc": {
          "line": 46,
          "column": 6
        }
      }
    },
    {
      "format": "python",
      "lines": 16,
      "fragment": "batch_size = 32\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128",
      "tokens": 0,
      "firstFile": {
        "name": "examples/imdb_lstm.py",
        "start": 28,
        "end": 43,
        "startLoc": {
          "line": 28,
          "column": 1
        },
        "endLoc": {
          "line": 43,
          "column": 38
        }
      },
      "secondFile": {
        "name": "examples/imdb_bidirectional_lstm.py",
        "start": 21,
        "end": 53,
        "startLoc": {
          "line": 21,
          "column": 1
        },
        "endLoc": {
          "line": 53,
          "column": 49
        }
      }
    },
    {
      "format": "python",
      "lines": 13,
      "fragment": "num_samples = 10000  # Number of samples to train on.\n# Path to the data txt file on disk.\ndata_path = 'fra-eng/fra.txt'\n\n# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nwith open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text,",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq.py",
        "start": 61,
        "end": 73,
        "startLoc": {
          "line": 61,
          "column": 1
        },
        "endLoc": {
          "line": 73,
          "column": 29
        }
      },
      "secondFile": {
        "name": "examples/cnn_seq2seq.py",
        "start": 56,
        "end": 68,
        "startLoc": {
          "line": 56,
          "column": 1
        },
        "endLoc": {
          "line": 68,
          "column": 30
        }
      }
    },
    {
      "format": "python",
      "lines": 45,
      "fragment": "= line.split('\\t')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Number of samples:', len(input_texts))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)\n\ninput_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.\n    encoder_input_data",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq.py",
        "start": 73,
        "end": 117,
        "startLoc": {
          "line": 73,
          "column": 32
        },
        "endLoc": {
          "line": 117,
          "column": 23
        }
      },
      "secondFile": {
        "name": "examples/cnn_seq2seq.py",
        "start": 68,
        "end": 112,
        "startLoc": {
          "line": 68,
          "column": 29
        },
        "endLoc": {
          "line": 112,
          "column": 8
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "]] = 1.\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n    decoder_input_data",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq.py",
        "start": 117,
        "end": 125,
        "startLoc": {
          "line": 117,
          "column": 56
        },
        "endLoc": {
          "line": 125,
          "column": 23
        }
      },
      "secondFile": {
        "name": "examples/cnn_seq2seq.py",
        "start": 111,
        "end": 120,
        "startLoc": {
          "line": 111,
          "column": 56
        },
        "endLoc": {
          "line": 120,
          "column": 43
        }
      }
    },
    {
      "format": "python",
      "lines": 43,
      "fragment": "input_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nwith open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text, _ = line.split('\\t')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Number of samples:', len(input_texts))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)\n\ninput_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype='float32')\n\nfor",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq_restore.py",
        "start": 29,
        "end": 71,
        "startLoc": {
          "line": 29,
          "column": 1
        },
        "endLoc": {
          "line": 71,
          "column": 4
        }
      },
      "secondFile": {
        "name": "examples/cnn_seq2seq.py",
        "start": 61,
        "end": 102,
        "startLoc": {
          "line": 61,
          "column": 1
        },
        "endLoc": {
          "line": 102,
          "column": 19
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": "]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())\n\n\n# Decodes an input sequence.  Future work should support beam search.",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq_restore.py",
        "start": 91,
        "end": 105,
        "startLoc": {
          "line": 91,
          "column": 31
        },
        "endLoc": {
          "line": 105,
          "column": 70
        }
      },
      "secondFile": {
        "name": "examples/lstm_seq2seq.py",
        "start": 175,
        "end": 189,
        "startLoc": {
          "line": 175,
          "column": 35
        },
        "endLoc": {
          "line": 189,
          "column": 4
        }
      }
    },
    {
      "format": "python",
      "lines": 46,
      "fragment": "def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\n\nfor seq_index in range(100):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)",
      "tokens": 0,
      "firstFile": {
        "name": "examples/lstm_seq2seq_restore.py",
        "start": 106,
        "end": 151,
        "startLoc": {
          "line": 106,
          "column": 1
        },
        "endLoc": {
          "line": 151,
          "column": 49
        }
      },
      "secondFile": {
        "name": "examples/lstm_seq2seq.py",
        "start": 189,
        "end": 234,
        "startLoc": {
          "line": 189,
          "column": 1
        },
        "endLoc": {
          "line": 234,
          "column": 49
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": ", 1)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Converts class vectors to binary class matrices.",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_hierarchical_rnn.py",
        "start": 57,
        "end": 66,
        "startLoc": {
          "line": 57,
          "column": 48
        },
        "endLoc": {
          "line": 66,
          "column": 51
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 33,
        "end": 43,
        "startLoc": {
          "line": 33,
          "column": 38
        },
        "endLoc": {
          "line": 43,
          "column": 49
        }
      }
    },
    {
      "format": "python",
      "lines": 14,
      "fragment": ", 1)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nprint",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_irnn.py",
        "start": 38,
        "end": 51,
        "startLoc": {
          "line": 38,
          "column": 44
        },
        "endLoc": {
          "line": 51,
          "column": 6
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 33,
        "end": 47,
        "startLoc": {
          "line": 33,
          "column": 38
        },
        "endLoc": {
          "line": 47,
          "column": 6
        }
      }
    },
    {
      "format": "python",
      "lines": 17,
      "fragment": "# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_mlp.py",
        "start": 20,
        "end": 36,
        "startLoc": {
          "line": 20,
          "column": 1
        },
        "endLoc": {
          "line": 36,
          "column": 6
        }
      },
      "secondFile": {
        "name": "examples/antirectifier.py",
        "start": 68,
        "end": 84,
        "startLoc": {
          "line": 68,
          "column": 1
        },
        "endLoc": {
          "line": 84,
          "column": 18
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "print(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Dense",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_mlp.py",
        "start": 29,
        "end": 37,
        "startLoc": {
          "line": 29,
          "column": 1
        },
        "endLoc": {
          "line": 37,
          "column": 16
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 40,
        "end": 48,
        "startLoc": {
          "line": 40,
          "column": 1
        },
        "endLoc": {
          "line": 48,
          "column": 17
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": "model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_mlp.py",
        "start": 49,
        "end": 56,
        "startLoc": {
          "line": 49,
          "column": 11
        },
        "endLoc": {
          "line": 56,
          "column": 34
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 63,
        "end": 70,
        "startLoc": {
          "line": 63,
          "column": 1
        },
        "endLoc": {
          "line": 70,
          "column": 34
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": ")\n    elif init == 'net2wider':\n        # add small noise to break symmetry, so that student model will have\n        # full capacity later\n        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_net2net.py",
        "start": 190,
        "end": 195,
        "startLoc": {
          "line": 190,
          "column": 65
        },
        "endLoc": {
          "line": 195,
          "column": 73
        }
      },
      "secondFile": {
        "name": "examples/mnist_net2net.py",
        "start": 140,
        "end": 145,
        "startLoc": {
          "line": 140,
          "column": 65
        },
        "endLoc": {
          "line": 145,
          "column": 73
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ", 3, input_shape=input_shape,\n                     padding='same', name='conv1'))\n    model.add(MaxPooling2D(2, name='pool1'))\n    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n    model.add(MaxPooling2D(2, name='pool2'))\n    model.add(Flatten(name='flatten'))\n    # a wider fc1 compared to teacher model",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_net2net.py",
        "start": 264,
        "end": 270,
        "startLoc": {
          "line": 264,
          "column": 37
        },
        "endLoc": {
          "line": 270,
          "column": 44
        }
      },
      "secondFile": {
        "name": "examples/mnist_net2net.py",
        "start": 234,
        "end": 240,
        "startLoc": {
          "line": 234,
          "column": 24
        },
        "endLoc": {
          "line": 240,
          "column": 10
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": "model = Sequential()\n    model.add(Conv2D(64, 3, input_shape=input_shape,\n                     padding='same', name='conv1'))\n    model.add(MaxPooling2D(2, name='pool1'))\n    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n    # add another conv2d layer to make original conv2 deeper",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_net2net.py",
        "start": 309,
        "end": 314,
        "startLoc": {
          "line": 309,
          "column": 5
        },
        "endLoc": {
          "line": 314,
          "column": 61
        }
      },
      "secondFile": {
        "name": "examples/mnist_net2net.py",
        "start": 233,
        "end": 238,
        "startLoc": {
          "line": 233,
          "column": 5
        },
        "endLoc": {
          "line": 238,
          "column": 10
        }
      }
    },
    {
      "format": "python",
      "lines": 17,
      "fragment": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n# convert class vectors to binary class matrices",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_sklearn_wrapper.py",
        "start": 24,
        "end": 40,
        "startLoc": {
          "line": 24,
          "column": 1
        },
        "endLoc": {
          "line": 40,
          "column": 49
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 24,
        "end": 39,
        "startLoc": {
          "line": 24,
          "column": 1
        },
        "endLoc": {
          "line": 39,
          "column": 6
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": ")\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# The size of the kernel used for the MaxPooling2D",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_swwae.py",
        "start": 107,
        "end": 116,
        "startLoc": {
          "line": 107,
          "column": 63
        },
        "endLoc": {
          "line": 116,
          "column": 51
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 33,
        "end": 43,
        "startLoc": {
          "line": 33,
          "column": 41
        },
        "endLoc": {
          "line": 43,
          "column": 49
        }
      }
    },
    {
      "format": "python",
      "lines": 11,
      "fragment": ")\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255\n    x_test /= 255\n    print('x_train shape:', x_train.shape)\n    print(x_train.shape[0], 'train samples')\n    print(x_test.shape[0], 'test samples')\n\n    # convert class vectors to binary class matrices\n    y_train = keras.utils.to_categorical(train",
      "tokens": 0,
      "firstFile": {
        "name": "examples/mnist_transfer_cnn.py",
        "start": 45,
        "end": 55,
        "startLoc": {
          "line": 45,
          "column": 63
        },
        "endLoc": {
          "line": 55,
          "column": 47
        }
      },
      "secondFile": {
        "name": "examples/mnist_cnn.py",
        "start": 33,
        "end": 44,
        "startLoc": {
          "line": 33,
          "column": 41
        },
        "endLoc": {
          "line": 44,
          "column": 45
        }
      }
    },
    {
      "format": "python",
      "lines": 16,
      "fragment": "def deprocess_image(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((3, img_nrows, img_ncols))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n\n# get tensor representations of our images",
      "tokens": 0,
      "firstFile": {
        "name": "examples/neural_style_transfer.py",
        "start": 110,
        "end": 125,
        "startLoc": {
          "line": 110,
          "column": 1
        },
        "endLoc": {
          "line": 125,
          "column": 43
        }
      },
      "secondFile": {
        "name": "examples/neural_doodle.py",
        "start": 114,
        "end": 130,
        "startLoc": {
          "line": 114,
          "column": 1
        },
        "endLoc": {
          "line": 130,
          "column": 4
        }
      }
    },
    {
      "format": "python",
      "lines": 14,
      "fragment": "if K.image_data_format() == 'channels_first':\n        a = K.square(\n            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n        b = K.square(\n            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n    else:\n        a = K.square(\n            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n        b = K.square(\n            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n\n\n# combine these loss functions into a single scalar",
      "tokens": 0,
      "firstFile": {
        "name": "examples/neural_style_transfer.py",
        "start": 194,
        "end": 207,
        "startLoc": {
          "line": 194,
          "column": 5
        },
        "endLoc": {
          "line": 207,
          "column": 52
        }
      },
      "secondFile": {
        "name": "examples/neural_doodle.py",
        "start": 285,
        "end": 298,
        "startLoc": {
          "line": 285,
          "column": 5
        },
        "endLoc": {
          "line": 298,
          "column": 75
        }
      }
    },
    {
      "format": "python",
      "lines": 17,
      "fragment": "], outputs)\n\n\ndef eval_loss_and_grads(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((1, 3, img_nrows, img_ncols))\n    else:\n        x = x.reshape((1, img_nrows, img_ncols, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype('float64')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype('float64')\n    return loss_value, grad_values\n\n# this Evaluator class makes it possible",
      "tokens": 0,
      "firstFile": {
        "name": "examples/neural_style_transfer.py",
        "start": 235,
        "end": 251,
        "startLoc": {
          "line": 235,
          "column": 42
        },
        "endLoc": {
          "line": 251,
          "column": 41
        }
      },
      "secondFile": {
        "name": "examples/neural_doodle.py",
        "start": 324,
        "end": 341,
        "startLoc": {
          "line": 324,
          "column": 37
        },
        "endLoc": {
          "line": 341,
          "column": 6
        }
      }
    },
    {
      "format": "python",
      "lines": 24,
      "fragment": "class Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\n\nevaluator = Evaluator()\n\n# run scipy-based optimization (L-BFGS) over the pixels of the generated image",
      "tokens": 0,
      "firstFile": {
        "name": "examples/neural_style_transfer.py",
        "start": 259,
        "end": 282,
        "startLoc": {
          "line": 259,
          "column": 1
        },
        "endLoc": {
          "line": 282,
          "column": 79
        }
      },
      "secondFile": {
        "name": "examples/neural_doodle.py",
        "start": 341,
        "end": 364,
        "startLoc": {
          "line": 341,
          "column": 1
        },
        "endLoc": {
          "line": 364,
          "column": 44
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ")\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=20)\n    print('Current loss value:', min_val)\n    # save current generated image\n    img = deprocess_image(x.copy())\n    fname = result_prefix",
      "tokens": 0,
      "firstFile": {
        "name": "examples/neural_style_transfer.py",
        "start": 287,
        "end": 294,
        "startLoc": {
          "line": 287,
          "column": 34
        },
        "endLoc": {
          "line": 294,
          "column": 26
        }
      },
      "secondFile": {
        "name": "examples/neural_doodle.py",
        "start": 371,
        "end": 378,
        "startLoc": {
          "line": 371,
          "column": 55
        },
        "endLoc": {
          "line": 378,
          "column": 30
        }
      }
    },
    {
      "format": "python",
      "lines": 24,
      "fragment": "print('Loading data...')\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nnum_classes = np.max(y_train) + 1\nprint(num_classes, 'classes')\n\nprint('Vectorizing sequence data...')\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Convert class vector to binary class matrix '\n      '(for use with categorical_crossentropy)')\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)\n\nprint('\\nBuilding network 1...'",
      "tokens": 0,
      "firstFile": {
        "name": "examples/reuters_mlp_relu_vs_selu.py",
        "start": 93,
        "end": 116,
        "startLoc": {
          "line": 93,
          "column": 1
        },
        "endLoc": {
          "line": 116,
          "column": 32
        }
      },
      "secondFile": {
        "name": "examples/reuters_mlp.py",
        "start": 17,
        "end": 40,
        "startLoc": {
          "line": 17,
          "column": 1
        },
        "endLoc": {
          "line": 40,
          "column": 26
        }
      }
    },
    {
      "format": "python",
      "lines": 37,
      "fragment": "encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 30x30 2D manifold of digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(10, 10))\n    start_range = digit_size // 2\n    end_range = n",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 69,
        "end": 105,
        "startLoc": {
          "line": 69,
          "column": 5
        },
        "endLoc": {
          "line": 105,
          "column": 18
        }
      },
      "secondFile": {
        "name": "examples/variational_autoencoder.py",
        "start": 67,
        "end": 103,
        "startLoc": {
          "line": 67,
          "column": 5
        },
        "endLoc": {
          "line": 103,
          "column": 18
        }
      }
    },
    {
      "format": "python",
      "lines": 18,
      "fragment": "* digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()\n\n\n# MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_size = x_train.shape[1]\nx_train",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 105,
        "end": 122,
        "startLoc": {
          "line": 105,
          "column": 19
        },
        "endLoc": {
          "line": 122,
          "column": 8
        }
      },
      "secondFile": {
        "name": "examples/variational_autoencoder.py",
        "start": 103,
        "end": 120,
        "startLoc": {
          "line": 103,
          "column": 25
        },
        "endLoc": {
          "line": 120,
          "column": 13
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": ") = mnist.load_data()\n\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 119,
        "end": 127,
        "startLoc": {
          "line": 119,
          "column": 36
        },
        "endLoc": {
          "line": 127,
          "column": 21
        }
      },
      "secondFile": {
        "name": "examples/mnist_denoising_autoencoder.py",
        "start": 36,
        "end": 44,
        "startLoc": {
          "line": 36,
          "column": 25
        },
        "endLoc": {
          "line": 44,
          "column": 67
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": ")\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n# use reparameterization trick to push the sampling out as input\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\nplot_model(encoder, to_file='vae_cnn_encoder.png'",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 152,
        "end": 163,
        "startLoc": {
          "line": 152,
          "column": 35
        },
        "endLoc": {
          "line": 163,
          "column": 50
        }
      },
      "secondFile": {
        "name": "examples/variational_autoencoder.py",
        "start": 136,
        "end": 147,
        "startLoc": {
          "line": 136,
          "column": 54
        },
        "endLoc": {
          "line": 147,
          "column": 50
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": ")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    help_ = \"Load h5 model trained weights\"\n    parser.add_argument(\"-w\", \"--weights\", help=help_)\n    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n    args = parser.parse_args()\n    models = (encoder, decoder)\n    data = (x_test, y_test)\n\n    # VAE loss = mse_loss or xent_loss + kl_loss\n    if args.mse:\n        reconstruction_loss = mse(K",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 191,
        "end": 205,
        "startLoc": {
          "line": 191,
          "column": 40
        },
        "endLoc": {
          "line": 205,
          "column": 36
        }
      },
      "secondFile": {
        "name": "examples/variational_autoencoder.py",
        "start": 161,
        "end": 177,
        "startLoc": {
          "line": 161,
          "column": 44
        },
        "endLoc": {
          "line": 177,
          "column": 41
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='rmsprop'",
      "tokens": 0,
      "firstFile": {
        "name": "examples/variational_autoencoder_deconv.py",
        "start": 211,
        "end": 216,
        "startLoc": {
          "line": 211,
          "column": 5
        },
        "endLoc": {
          "line": 216,
          "column": 36
        }
      },
      "secondFile": {
        "name": "examples/variational_autoencoder.py",
        "start": 183,
        "end": 188,
        "startLoc": {
          "line": 183,
          "column": 5
        },
        "endLoc": {
          "line": 188,
          "column": 33
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": ")\n\n\ndef get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret regularizer identifier: '",
      "tokens": 0,
      "firstFile": {
        "name": "keras/regularizers.py",
        "start": 73,
        "end": 87,
        "startLoc": {
          "line": 73,
          "column": 72
        },
        "endLoc": {
          "line": 87,
          "column": 72
        }
      },
      "secondFile": {
        "name": "keras/constraints.py",
        "start": 168,
        "end": 182,
        "startLoc": {
          "line": 168,
          "column": 71
        },
        "endLoc": {
          "line": 182,
          "column": 71
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": ")\n    x = Input(shape=(1,))\n    y = inner_model(x)\n    outer_model = Model(x, y)\n    assert outer_model.trainable_weights == inner_model.trainable_weights\n    inner_model.trainable = False\n    assert outer_model.trainable_weights == []\n    inner_model.trainable = True\n    inner_model.layers[-1].trainable = False\n    assert outer_model.trainable_weights == []\n\n    # a Model inside a Sequential",
      "tokens": 0,
      "firstFile": {
        "name": "tests/test_dynamic_trainability.py",
        "start": 84,
        "end": 95,
        "startLoc": {
          "line": 84,
          "column": 29
        },
        "endLoc": {
          "line": 95,
          "column": 34
        }
      },
      "secondFile": {
        "name": "tests/test_dynamic_trainability.py",
        "start": 57,
        "end": 69,
        "startLoc": {
          "line": 57,
          "column": 42
        },
        "endLoc": {
          "line": 69,
          "column": 39
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": ")\n    outer_model = Sequential()\n    outer_model.add(inner_model)\n    assert outer_model.trainable_weights == inner_model.trainable_weights\n    inner_model.trainable = False\n    assert outer_model.trainable_weights == []\n    inner_model.trainable = True\n    inner_model.layers[-1].trainable = False\n    assert outer_model.trainable_weights == []\n\n\nif",
      "tokens": 0,
      "firstFile": {
        "name": "tests/test_dynamic_trainability.py",
        "start": 98,
        "end": 109,
        "startLoc": {
          "line": 98,
          "column": 29
        },
        "endLoc": {
          "line": 109,
          "column": 3
        }
      },
      "secondFile": {
        "name": "tests/test_dynamic_trainability.py",
        "start": 71,
        "end": 81,
        "startLoc": {
          "line": 71,
          "column": 42
        },
        "endLoc": {
          "line": 81,
          "column": 29
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": "():\n    model = create_sequential_model()\n    model.compile(loss=loss, optimizer='rmsprop')\n\n    ((x_train, y_train), (x_test, y_test),\n     (sample_weight, class_weight, test_ids)) = _get_test_data()\n\n    model.fit(x_train, y_train, batch_size=batch_size,\n              epochs=epochs // 3, verbose=0,\n              sample_weight",
      "tokens": 0,
      "firstFile": {
        "name": "tests/test_loss_weighting.py",
        "start": 99,
        "end": 108,
        "startLoc": {
          "line": 99,
          "column": 35
        },
        "endLoc": {
          "line": 108,
          "column": 28
        }
      },
      "secondFile": {
        "name": "tests/test_loss_weighting.py",
        "start": 74,
        "end": 83,
        "startLoc": {
          "line": 74,
          "column": 34
        },
        "endLoc": {
          "line": 83,
          "column": 27
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "])\n    x = np.random.random((1, 3))\n    y = np.random.random((1, 3))\n    model.train_on_batch(x, y)\n\n    out = model.predict(x)\n    state = pickle.dumps(model)\n\n    model",
      "tokens": 0,
      "firstFile": {
        "name": "tests/test_model_pickling.py",
        "start": 87,
        "end": 95,
        "startLoc": {
          "line": 87,
          "column": 56
        },
        "endLoc": {
          "line": 95,
          "column": 10
        }
      },
      "secondFile": {
        "name": "tests/test_model_pickling.py",
        "start": 61,
        "end": 71,
        "startLoc": {
          "line": 61,
          "column": 73
        },
        "endLoc": {
          "line": 71,
          "column": 9
        }
      }
    },
    {
      "format": "markdown",
      "lines": 13,
      "fragment": "backend.\n\n---\n\nPlease make sure that the boxes below are checked before you submit your issue.\nIf your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.\n\nThank you!\n\n- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:\n`pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps`\n\n- [ ] Check that your",
      "tokens": 0,
      "firstFile": {
        "name": ".github/ISSUE_TEMPLATE/c--cntk-backend-users.md",
        "start": 3,
        "end": 15,
        "startLoc": {
          "line": 3,
          "column": 56
        },
        "endLoc": {
          "line": 15,
          "column": 22
        }
      },
      "secondFile": {
        "name": ".github/ISSUE_TEMPLATE/b--theano-backend-users.md",
        "start": 3,
        "end": 15,
        "startLoc": {
          "line": 3,
          "column": 58
        },
        "endLoc": {
          "line": 15,
          "column": 21
        }
      }
    },
    {
      "format": "markdown",
      "lines": 7,
      "fragment": "\n```\n\n- __Returns:__\n    - 2 tuples:\n        - __x_train, x_test__: uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the `image_data_format` backend setting of either `channels_first` or `channels_last` respectively.\n        - __y_train, y_test__: uint8 array of category labels with",
      "tokens": 0,
      "firstFile": {
        "name": "docs/templates/datasets.md",
        "start": 32,
        "end": 38,
        "startLoc": {
          "line": 32,
          "column": 77
        },
        "endLoc": {
          "line": 38,
          "column": 67
        }
      },
      "secondFile": {
        "name": "docs/templates/datasets.md",
        "start": 12,
        "end": 18,
        "startLoc": {
          "line": 12,
          "column": 59
        },
        "endLoc": {
          "line": 18,
          "column": 72
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": ")\n\n    y_train = np.reshape(y_train, (len(y_train), 1))\n    y_test = np.reshape(y_test, (len(y_test), 1))\n\n    if K.image_data_format() == 'channels_last':\n        x_train = x_train.transpose(0, 2, 3, 1)\n        x_test = x_test.transpose(0, 2, 3, 1)\n\n    return (x_train, y_train), (x_test, y_test)",
      "tokens": 0,
      "firstFile": {
        "name": "keras/datasets/cifar100.py",
        "start": 37,
        "end": 46,
        "startLoc": {
          "line": 37,
          "column": 72
        },
        "endLoc": {
          "line": 46,
          "column": 48
        }
      },
      "secondFile": {
        "name": "keras/datasets/cifar10.py",
        "start": 35,
        "end": 44,
        "startLoc": {
          "line": 35,
          "column": 38
        },
        "endLoc": {
          "line": 44,
          "column": 48
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "if start_char is not None:\n        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n    elif index_from:\n        xs = [[w + index_from for w in x] for x in xs]\n\n    if maxlen:\n        xs, labels = _remove_long_seq(maxlen, xs, labels)\n\n    if not num_words",
      "tokens": 0,
      "firstFile": {
        "name": "keras/datasets/reuters.py",
        "start": 65,
        "end": 73,
        "startLoc": {
          "line": 65,
          "column": 5
        },
        "endLoc": {
          "line": 73,
          "column": 21
        }
      },
      "secondFile": {
        "name": "keras/datasets/imdb.py",
        "start": 76,
        "end": 83,
        "startLoc": {
          "line": 76,
          "column": 5
        },
        "endLoc": {
          "line": 83,
          "column": 18
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ")\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_test, y_test)\n\n\ndef get_word_index(path='reuters_word_index.json'",
      "tokens": 0,
      "firstFile": {
        "name": "keras/datasets/reuters.py",
        "start": 84,
        "end": 91,
        "startLoc": {
          "line": 84,
          "column": 41
        },
        "endLoc": {
          "line": 91,
          "column": 50
        }
      },
      "secondFile": {
        "name": "keras/datasets/imdb.py",
        "start": 100,
        "end": 107,
        "startLoc": {
          "line": 100,
          "column": 22
        },
        "endLoc": {
          "line": 107,
          "column": 47
        }
      }
    },
    {
      "format": "python",
      "lines": 11,
      "fragment": "num_samples = check_num_samples(ins,\n                                    batch_size=batch_size,\n                                    steps=steps,\n                                    steps_name='steps')\n\n    # Check if callbacks have not been already configured\n    if not isinstance(callbacks, cbks.CallbackList):\n        callbacks = cbks.CallbackList(callbacks)\n        callback_model = model._get_callback_model()\n        callbacks.set_model(callback_model)\n        callback_metrics",
      "tokens": 0,
      "firstFile": {
        "name": "keras/engine/training_arrays.py",
        "start": 369,
        "end": 379,
        "startLoc": {
          "line": 369,
          "column": 5
        },
        "endLoc": {
          "line": 379,
          "column": 25
        }
      },
      "secondFile": {
        "name": "keras/engine/training_arrays.py",
        "start": 247,
        "end": 257,
        "startLoc": {
          "line": 247,
          "column": 5
        },
        "endLoc": {
          "line": 257,
          "column": 24
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": "isinstance(ins[-1], int):\n                # Do not slice the training phase flag.\n                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n            else:\n                ins_batch = slice_arrays(ins, batch_ids)\n            for i in indices_for_conversion_to_dense:\n                ins_batch[i] = ins_batch[i].toarray()\n\n            batch_logs = {'batch': batch_index, 'size': len(batch_ids)}\n            callbacks._call_batch_hook('test'",
      "tokens": 0,
      "firstFile": {
        "name": "keras/engine/training_arrays.py",
        "start": 439,
        "end": 448,
        "startLoc": {
          "line": 439,
          "column": 16
        },
        "endLoc": {
          "line": 448,
          "column": 46
        }
      },
      "secondFile": {
        "name": "keras/engine/training_arrays.py",
        "start": 314,
        "end": 323,
        "startLoc": {
          "line": 314,
          "column": 24
        },
        "endLoc": {
          "line": 323,
          "column": 49
        }
      }
    },
    {
      "format": "python",
      "lines": 14,
      "fragment": ",\n                 padding='valid',\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(LocallyConnected2D",
      "tokens": 0,
      "firstFile": {
        "name": "keras/layers/local.py",
        "start": 268,
        "end": 281,
        "startLoc": {
          "line": 268,
          "column": 32
        },
        "endLoc": {
          "line": 281,
          "column": 33
        }
      },
      "secondFile": {
        "name": "keras/layers/local.py",
        "start": 84,
        "end": 97,
        "startLoc": {
          "line": 84,
          "column": 27
        },
        "endLoc": {
          "line": 97,
          "column": 33
        }
      }
    },
    {
      "format": "python",
      "lines": 12,
      "fragment": "'(only \"valid\" is supported): ' + padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=4",
      "tokens": 0,
      "firstFile": {
        "name": "keras/layers/local.py",
        "start": 288,
        "end": 299,
        "startLoc": {
          "line": 288,
          "column": 30
        },
        "endLoc": {
          "line": 299,
          "column": 43
        }
      },
      "secondFile": {
        "name": "keras/layers/local.py",
        "start": 104,
        "end": 115,
        "startLoc": {
          "line": 104,
          "column": 30
        },
        "endLoc": {
          "line": 115,
          "column": 43
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": ",\n            self.filters)\n        self.kernel = self.add_weight(shape=self.kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(output_row",
      "tokens": 0,
      "firstFile": {
        "name": "keras/layers/local.py",
        "start": 321,
        "end": 329,
        "startLoc": {
          "line": 321,
          "column": 69
        },
        "endLoc": {
          "line": 329,
          "column": 58
        }
      },
      "secondFile": {
        "name": "keras/layers/local.py",
        "start": 127,
        "end": 137,
        "startLoc": {
          "line": 127,
          "column": 61
        },
        "endLoc": {
          "line": 137,
          "column": 37
        }
      }
    },
    {
      "format": "python",
      "lines": 13,
      "fragment": ",\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer':\n                regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(LocallyConnected2D",
      "tokens": 0,
      "firstFile": {
        "name": "keras/layers/local.py",
        "start": 380,
        "end": 392,
        "startLoc": {
          "line": 380,
          "column": 44
        },
        "endLoc": {
          "line": 392,
          "column": 47
        }
      },
      "secondFile": {
        "name": "keras/layers/local.py",
        "start": 167,
        "end": 179,
        "startLoc": {
          "line": 167,
          "column": 36
        },
        "endLoc": {
          "line": 179,
          "column": 47
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": "10, 10, 3))\n    xint = x.astype('int32')\n    assert utils.preprocess_input(x).shape == x.shape\n    assert utils.preprocess_input(xint).shape == xint.shape\n\n    out1 = utils.preprocess_input(x, 'channels_last')\n    out1int = utils.preprocess_input(xint, 'channels_last')\n    out2 = utils.preprocess_input(np.transpose(x, (2",
      "tokens": 0,
      "firstFile": {
        "name": "tests/integration_tests/imagenet_utils_test.py",
        "start": 27,
        "end": 34,
        "startLoc": {
          "line": 27,
          "column": 36
        },
        "endLoc": {
          "line": 34,
          "column": 53
        }
      },
      "secondFile": {
        "name": "tests/integration_tests/imagenet_utils_test.py",
        "start": 12,
        "end": 19,
        "startLoc": {
          "line": 12,
          "column": 39
        },
        "endLoc": {
          "line": 19,
          "column": 53
        }
      }
    },
    {
      "format": "python",
      "lines": 23,
      "fragment": "input_shape = (16, 16, 3)\n    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n                                                         num_test=200,\n                                                         input_shape=input_shape,\n                                                         classification=True,\n                                                         num_classes=4)\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n\n    model = Sequential([\n        layers.Conv2D(filters=8, kernel_size=3,\n                      activation='relu',\n                      input_shape=input_shape),\n        layers.MaxPooling2D(pool_size=2),\n        layers.Conv2D(filters=4, kernel_size=(3, 3),\n                      activation='relu', padding='same'),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(y_test.shape[-1], activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    history",
      "tokens": 0,
      "firstFile": {
        "name": "tests/integration_tests/test_image_data_tasks.py",
        "start": 48,
        "end": 70,
        "startLoc": {
          "line": 48,
          "column": 5
        },
        "endLoc": {
          "line": 70,
          "column": 12
        }
      },
      "secondFile": {
        "name": "tests/integration_tests/test_image_data_tasks.py",
        "start": 14,
        "end": 36,
        "startLoc": {
          "line": 14,
          "column": 5
        },
        "endLoc": {
          "line": 36,
          "column": 10
        }
      }
    },
    {
      "format": "python",
      "lines": 17,
      "fragment": "():\n    '''\n    Classify temporal sequences of float numbers\n    of length 3 into 2 classes using\n    single layer of GRU units and softmax applied\n    to the last activations of the units\n    '''\n    np.random.seed(1337)\n    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n                                                         num_test=20,\n                                                         input_shape=(3, 4),\n                                                         classification=True,\n                                                         num_classes=2)\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n\n    inputs",
      "tokens": 0,
      "firstFile": {
        "name": "tests/integration_tests/test_temporal_data_tasks.py",
        "start": 46,
        "end": 62,
        "startLoc": {
          "line": 46,
          "column": 44
        },
        "endLoc": {
          "line": 62,
          "column": 11
        }
      },
      "secondFile": {
        "name": "tests/integration_tests/test_temporal_data_tasks.py",
        "start": 14,
        "end": 30,
        "startLoc": {
          "line": 14,
          "column": 33
        },
        "endLoc": {
          "line": 30,
          "column": 10
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": ", rtol=1e-05)\n\n    negative_values = np.array([[-1, -2]], dtype=K.floatx())\n\n    result = f([negative_values])[0]\n    true_result = (np.exp(negative_values) - 1) *",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/activations_test.py",
        "start": 219,
        "end": 224,
        "startLoc": {
          "line": 219,
          "column": 52
        },
        "endLoc": {
          "line": 224,
          "column": 50
        }
      },
      "secondFile": {
        "name": "tests/keras/activations_test.py",
        "start": 202,
        "end": 206,
        "startLoc": {
          "line": 202,
          "column": 40
        },
        "endLoc": {
          "line": 206,
          "column": 50
        }
      }
    },
    {
      "format": "unknown",
      "lines": 111,
      "fragment": "@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_normal(tensor_shape):\n    _runner(initializers.RandomNormal(mean=0, stddev=1), tensor_shape,\n            target_mean=0., target_std=1)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_truncated_normal(tensor_shape):\n    _runner(initializers.TruncatedNormal(mean=0, stddev=1), tensor_shape,\n            target_mean=0., target_max=2, target_min=-2)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_constant(tensor_shape):\n    _runner(initializers.Constant(2), tensor_shape,\n            target_mean=2, target_max=2, target_min=2)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_lecun_uniform(tensor_shape):\n    fan_in, _ = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(1. / fan_in)\n    _runner(initializers.lecun_uniform(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_glorot_uniform(tensor_shape):\n    fan_in, fan_out = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(2. / (fan_in + fan_out))\n    _runner(initializers.glorot_uniform(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_he_uniform(tensor_shape):\n    fan_in, _ = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(2. / fan_in)\n    _runner(initializers.he_uniform(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_lecun_normal(tensor_shape):\n    fan_in, _ = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(1. / fan_in)\n    _runner(initializers.lecun_normal(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_glorot_normal(tensor_shape):\n    fan_in, fan_out = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(2. / (fan_in + fan_out))\n    _runner(initializers.glorot_normal(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_he_normal(tensor_shape):\n    fan_in, _ = initializers._compute_fans(tensor_shape)\n    std = np.sqrt(2. / fan_in)\n    _runner(initializers.he_normal(), tensor_shape,\n            target_mean=0., target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_orthogonal(tensor_shape):\n    _runner(initializers.orthogonal(), tensor_shape,\n            target_mean=0.)\n\n\ndef test_orthogonal_init_does_not_affect_global_rng():\n    np.random.seed(1337)\n    before = np.random.randint(0, 100, size=10)\n\n    np.random.seed(1337)\n    init = initializers.orthogonal(seed=9876)\n    init(shape=(10, 5))\n    after = np.random.randint(0, 100, size=10)\n\n    assert np.array_equal(before, after)\n\n\n@pytest.mark.parametrize('tensor_shape',\n                         [(100, 100), (10, 20), (30, 80), (1, 2, 3, 4)],\n                         ids=['FC', 'RNN', 'RNN_INVALID', 'CONV'])\ndef test_identity(tensor_shape):\n    target_mean = (1. * min(tensor_shape)) / (tensor_shape[0] * tensor_shape[1])\n    if len(tensor_shape) > 2:\n        with pytest.raises(ValueError):\n            _runner(initializers.identity(), tensor_shape,\n                    target_mean=target_mean, target_max=1.)\n    else:\n        _runner(initializers.identity(), tensor_shape,\n                target_mean=target_mean, target_max=1.)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_zero(tensor_shape):\n    _runner(initializers.zeros(), tensor_shape,\n            target_mean=0., target_max=0.)\n\n\n@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\ndef test_one(tensor_shape):\n    _runner(initializers.ones(), tensor_shape,\n            target_mean=1., target_max=1.)\n\n\n@pytest.mark.parametrize",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/initializers_test.py",
        "start": 35,
        "end": 145,
        "startLoc": {
          "line": 35,
          "column": 1
        },
        "endLoc": {
          "line": 145,
          "column": 25
        }
      },
      "secondFile": {
        "name": "tests/keras/initializers_test.py",
        "start": 29,
        "end": 101,
        "startLoc": {
          "line": 29,
          "column": 1
        },
        "endLoc": {
          "line": 101,
          "column": 25
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": "([self.x, self.x], [self.y1, self.y2],\n                                     sample_weight={\n                                         'output_1': self.sample_weight_1,\n                                         'output_2': self.sample_weight_2})\n        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n\n        # Set weights for one output.\n        result = model.test_on_batch",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/metrics_correctness_test.py",
        "start": 296,
        "end": 303,
        "startLoc": {
          "line": 296,
          "column": 37
        },
        "endLoc": {
          "line": 303,
          "column": 37
        }
      },
      "secondFile": {
        "name": "tests/keras/metrics_correctness_test.py",
        "start": 259,
        "end": 266,
        "startLoc": {
          "line": 259,
          "column": 38
        },
        "endLoc": {
          "line": 266,
          "column": 38
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ")\n    model.add(Activation('relu'))\n    model.add(Dense(y_train.shape[1]))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    model",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/optimizers_test.py",
        "start": 55,
        "end": 62,
        "startLoc": {
          "line": 55,
          "column": 20
        },
        "endLoc": {
          "line": 62,
          "column": 10
        }
      },
      "secondFile": {
        "name": "tests/keras/optimizers_test.py",
        "start": 33,
        "end": 41,
        "startLoc": {
          "line": 33,
          "column": 57
        },
        "endLoc": {
          "line": 41,
          "column": 12
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ")\n    model = Sequential()\n    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n    model.add(Activation('relu'))\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n    model.load_weights",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 156,
        "end": 163,
        "startLoc": {
          "line": 156,
          "column": 45
        },
        "endLoc": {
          "line": 163,
          "column": 23
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 119,
        "end": 128,
        "startLoc": {
          "line": 119,
          "column": 40
        },
        "endLoc": {
          "line": 128,
          "column": 14
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": ")\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n              validation_data=(x_test, y_test))\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n              validation_split=0.1)\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n              shuffle=False)\n\n    model.train_on_batch(x_train[:32], y_train[:32])\n\n    loss",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 195,
        "end": 209,
        "startLoc": {
          "line": 195,
          "column": 21
        },
        "endLoc": {
          "line": 209,
          "column": 9
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 124,
        "end": 138,
        "startLoc": {
          "line": 124,
          "column": 33
        },
        "endLoc": {
          "line": 138,
          "column": 12
        }
      }
    },
    {
      "format": "python",
      "lines": 15,
      "fragment": ")\n\n    inner = Sequential()\n    inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n    inner.add(Activation('relu'))\n    inner.add(Dense(num_classes))\n\n    middle = Sequential()\n    middle.add(inner)\n\n    model = Sequential()\n    model.add(middle)\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n    model.load_weights",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 216,
        "end": 230,
        "startLoc": {
          "line": 216,
          "column": 45
        },
        "endLoc": {
          "line": 230,
          "column": 23
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 184,
        "end": 199,
        "startLoc": {
          "line": 184,
          "column": 59
        },
        "endLoc": {
          "line": 199,
          "column": 14
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ")\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n    model.load_weights(fname)\n    os.remove(fname)\n\n    nloss = model.evaluate(x_test, y_test, verbose=0)\n    assert(loss",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 227,
        "end": 234,
        "startLoc": {
          "line": 227,
          "column": 21
        },
        "endLoc": {
          "line": 234,
          "column": 16
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 160,
        "end": 167,
        "startLoc": {
          "line": 160,
          "column": 33
        },
        "endLoc": {
          "line": 167,
          "column": 19
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": "assert model.built is False\n    assert len(model.layers) == 2\n    assert len(model.weights) == 0\n\n    model.train_on_batch(\n        np.random.random((2, 4)), np.random.random((2, 5",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 470,
        "end": 475,
        "startLoc": {
          "line": 470,
          "column": 5
        },
        "endLoc": {
          "line": 475,
          "column": 57
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 437,
        "end": 442,
        "startLoc": {
          "line": 437,
          "column": 5
        },
        "endLoc": {
          "line": 442,
          "column": 57
        }
      }
    },
    {
      "format": "python",
      "lines": 10,
      "fragment": "to_categorical(y_test)\n\n    model = Sequential([\n        layers.Conv2D(filters=8, kernel_size=3,\n                      activation='relu',\n                      input_shape=input_shape),\n        layers.MaxPooling2D(pool_size=2),\n        layers.Conv2D(filters=4, kernel_size=(3, 3),\n                      activation='relu', padding='same'),\n        layers.BatchNormalization",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/callbacks/tensorboard_test.py",
        "start": 218,
        "end": 227,
        "startLoc": {
          "line": 218,
          "column": 23
        },
        "endLoc": {
          "line": 227,
          "column": 34
        }
      },
      "secondFile": {
        "name": "tests/integration_tests/test_image_data_tasks.py",
        "start": 21,
        "end": 30,
        "startLoc": {
          "line": 21,
          "column": 14
        },
        "endLoc": {
          "line": 30,
          "column": 38
        }
      }
    },
    {
      "format": "python",
      "lines": 23,
      "fragment": "(\n        weights=[\n            layer.kernel_i,\n            layer.kernel_f,\n            layer.kernel_c,\n            layer.kernel_o,\n            layer.recurrent_kernel_i,\n            layer.recurrent_kernel_f,\n            layer.recurrent_kernel_c,\n            layer.recurrent_kernel_o,\n        ],\n        biases=[\n            layer.bias_i_i,\n            layer.bias_f_i,\n            layer.bias_c_i,\n            layer.bias_o_i,\n            layer.bias_i,\n            layer.bias_f,\n            layer.bias_c,\n            layer.bias_o,\n        ],\n    )\n    ref_params_value",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 45,
        "end": 67,
        "startLoc": {
          "line": 45,
          "column": 55
        },
        "endLoc": {
          "line": 67,
          "column": 21
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 23,
        "end": 45,
        "startLoc": {
          "line": 23,
          "column": 40
        },
        "endLoc": {
          "line": 45,
          "column": 15
        }
      }
    },
    {
      "format": "python",
      "lines": 19,
      "fragment": "(\n        weights=[\n            layer.kernel_r,\n            layer.kernel_z,\n            layer.kernel_h,\n            layer.recurrent_kernel_r,\n            layer.recurrent_kernel_z,\n            layer.recurrent_kernel_h,\n        ],\n        biases=[\n            layer.bias_r_i,\n            layer.bias_z_i,\n            layer.bias_h_i,\n            layer.bias_r,\n            layer.bias_z,\n            layer.bias_h,\n        ],\n    )\n    ref_params_value",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 98,
        "end": 116,
        "startLoc": {
          "line": 98,
          "column": 40
        },
        "endLoc": {
          "line": 116,
          "column": 21
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 80,
        "end": 98,
        "startLoc": {
          "line": 80,
          "column": 54
        },
        "endLoc": {
          "line": 98,
          "column": 11
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "():\n    input_size = 10\n    timesteps = 6\n    units = 2\n    num_samples = 32\n    for layer_class in [keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM]:\n        num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1\n\n        inputs = keras.Input((",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 257,
        "end": 265,
        "startLoc": {
          "line": 257,
          "column": 44
        },
        "endLoc": {
          "line": 265,
          "column": 31
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/cudnn_recurrent_test.py",
        "start": 234,
        "end": 243,
        "startLoc": {
          "line": 234,
          "column": 22
        },
        "endLoc": {
          "line": 243,
          "column": 41
        }
      }
    },
    {
      "format": "python",
      "lines": 8,
      "fragment": ".output_shape == (None, 4, 5)\n\n    x1 = np.random.random((2, 4, 5))\n    x2 = np.random.random((2, 4, 5))\n    x3 = np.random.random((2, 4, 5))\n    out = model.predict([x1, x2, x3])\n    assert out.shape == (2, 4, 5)\n    assert_allclose(out, x1 *",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 87,
        "end": 94,
        "startLoc": {
          "line": 87,
          "column": 21
        },
        "endLoc": {
          "line": 94,
          "column": 30
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 21,
        "end": 28,
        "startLoc": {
          "line": 21,
          "column": 21
        },
        "endLoc": {
          "line": 28,
          "column": 30
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ".output_shape == (None, 4, 5)\n\n    x1 = np.random.random((2, 4, 5))\n    x2 = np.random.random((2, 4, 5))\n    out = model.predict([x1, x2])\n    assert out.shape == (2, 4, 5)\n    assert_allclose(out, 0.5",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 106,
        "end": 112,
        "startLoc": {
          "line": 106,
          "column": 21
        },
        "endLoc": {
          "line": 112,
          "column": 29
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 54,
        "end": 60,
        "startLoc": {
          "line": 54,
          "column": 26
        },
        "endLoc": {
          "line": 60,
          "column": 28
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ".output_shape == (None, 4, 5)\n\n    x1 = np.random.random((2, 4, 5))\n    x2 = np.random.random((2, 4, 5))\n    out = model.predict([x1, x2])\n    assert out.shape == (2, 4, 5)\n    assert_allclose(out, np",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 124,
        "end": 130,
        "startLoc": {
          "line": 124,
          "column": 21
        },
        "endLoc": {
          "line": 130,
          "column": 28
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 54,
        "end": 60,
        "startLoc": {
          "line": 54,
          "column": 26
        },
        "endLoc": {
          "line": 60,
          "column": 28
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "()\n    o2 = max_layer([i1, i2])\n    assert max_layer.output_shape == (None, 4, 5)\n\n    x1 = np.random.random((2, 4, 5))\n    x2 = np.random.random((2, 4, 5))\n    out = model.predict([x1, x2])\n    assert out.shape == (2, 4, 5)\n    assert_allclose(out, np.minimum",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 140,
        "end": 148,
        "startLoc": {
          "line": 140,
          "column": 31
        },
        "endLoc": {
          "line": 148,
          "column": 36
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 122,
        "end": 130,
        "startLoc": {
          "line": 122,
          "column": 31
        },
        "endLoc": {
          "line": 130,
          "column": 36
        }
      }
    },
    {
      "format": "python",
      "lines": 6,
      "fragment": ", 5)\n\n    x1 = np.random.random((2, 4, 5))\n    x2 = np.random.random((2, 4, 5))\n    out = model.predict([x1, x2])\n    assert out.shape == (2, 8",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 166,
        "end": 171,
        "startLoc": {
          "line": 166,
          "column": 49
        },
        "endLoc": {
          "line": 171,
          "column": 30
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 54,
        "end": 59,
        "startLoc": {
          "line": 54,
          "column": 51
        },
        "endLoc": {
          "line": 59,
          "column": 30
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": ")\n        model = models.Model([i1, i2], o)\n\n        x1 = np.random.random((2, 4, 5))\n        x2 = np.random.random((2, 5))\n        out = model.predict([x1, x2])\n        assert out.shape == (2, 4, 5)\n\n    # ndim not provided",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 253,
        "end": 261,
        "startLoc": {
          "line": 253,
          "column": 51
        },
        "endLoc": {
          "line": 261,
          "column": 24
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 239,
        "end": 247,
        "startLoc": {
          "line": 239,
          "column": 45
        },
        "endLoc": {
          "line": 247,
          "column": 26
        }
      }
    },
    {
      "format": "python",
      "lines": 13,
      "fragment": "i1 = layers.Input(shape=(None, None))\n        i2 = layers.Input(shape=(None,))\n        ops = [layers.add, layers.maximum]\n        for op in ops:\n            o = op([i1, i2])\n            assert o._keras_shape == (None, None, None)\n            model = models.Model([i1, i2], o)\n\n            x1 = np.random.random((2, 4, 5))\n            x2 = np.random.random((2, 5))\n            out = model.predict([x1, x2])\n            assert out.shape == (2, 4, 5)\n        K",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 266,
        "end": 278,
        "startLoc": {
          "line": 266,
          "column": 9
        },
        "endLoc": {
          "line": 278,
          "column": 10
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/merge_test.py",
        "start": 248,
        "end": 247,
        "startLoc": {
          "line": 248,
          "column": 5
        },
        "endLoc": {
          "line": 247,
          "column": 26
        }
      }
    },
    {
      "format": "python",
      "lines": 11,
      "fragment": ",\n                                            input_shape=(3, 4, 4), momentum=0.8)\n    model.add(norm)\n    model.compile(loss='mse', optimizer='sgd')\n\n    # centered on 5.0, variance 10.0\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n    model.fit(x, x, epochs=4, verbose=0)\n    out = model.predict(x)\n\n    assert_allclose",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/normalization_test.py",
        "start": 145,
        "end": 155,
        "startLoc": {
          "line": 145,
          "column": 79
        },
        "endLoc": {
          "line": 155,
          "column": 20
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/normalization_test.py",
        "start": 124,
        "end": 133,
        "startLoc": {
          "line": 124,
          "column": 51
        },
        "endLoc": {
          "line": 133,
          "column": 8
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ")\n\n    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\n    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n\n\ndef",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/normalization_test.py",
        "start": 153,
        "end": 159,
        "startLoc": {
          "line": 153,
          "column": 26
        },
        "endLoc": {
          "line": 159,
          "column": 4
        }
      },
      "secondFile": {
        "name": "tests/keras/layers/normalization_test.py",
        "start": 134,
        "end": 140,
        "startLoc": {
          "line": 134,
          "column": 55
        },
        "endLoc": {
          "line": 140,
          "column": 21
        }
      }
    },
    {
      "format": "python",
      "lines": 22,
      "fragment": ")\n\n    model.trainable = False\n    assert not model.updates\n\n    model.compile('sgd', 'mse')\n    assert not model.updates\n\n    x1 = model.predict(val_a)\n    model.train_on_batch(val_a, val_out)\n    x2 = model.predict(val_a)\n    assert_allclose(x1, x2, atol=1e-7)\n\n    model.trainable = True\n    model.compile('sgd', 'mse')\n    assert model.updates\n\n    model.train_on_batch(val_a, val_out)\n    x2 = model.predict(val_a)\n    assert np.abs(np.sum(x1 - x2)) > 1e-5\n\n    layer",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/layers/normalization_test.py",
        "start": 193,
        "end": 214,
        "startLoc": {
          "line": 193,
          "column": 23
        },
        "endLoc": {
          "line": 214,
          "column": 10
        }
      },
      "secondFile": {
        "name": "tests/keras/test_sequential_model.py",
        "start": 409,
        "end": 431,
        "startLoc": {
          "line": 409,
          "column": 64
        },
        "endLoc": {
          "line": 431,
          "column": 4
        }
      }
    },
    {
      "format": "python",
      "lines": 9,
      "fragment": "'W_regularizer': regularizers.l2(0.01),\n                       'b_regularizer': regularizers.l1(0.01),\n                       'activity_regularizer': regularizers.l2(0.01),\n                       'W_constraint': constraints.MaxNorm(1),\n                       'b_constraint': constraints.MaxNorm(1)},\n               input_shape=(3, 2))\n\n\nif",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/legacy/layers_test.py",
        "start": 30,
        "end": 38,
        "startLoc": {
          "line": 30,
          "column": 24
        },
        "endLoc": {
          "line": 38,
          "column": 3
        }
      },
      "secondFile": {
        "name": "tests/keras/legacy/layers_test.py",
        "start": 15,
        "end": 23,
        "startLoc": {
          "line": 15,
          "column": 24
        },
        "endLoc": {
          "line": 23,
          "column": 4
        }
      }
    },
    {
      "format": "python",
      "lines": 11,
      "fragment": "():\n    data = np.random.random((3, 5))\n\n    def load_function(h5file_):\n        return h5file_['data'][:]\n\n    with temp_filename('.h5') as fname:\n        with h5py.File(fname, 'w') as h5file:\n            h5file['data'] = data\n\n        file_like",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/utils/io_utils_test.py",
        "start": 305,
        "end": 315,
        "startLoc": {
          "line": 305,
          "column": 45
        },
        "endLoc": {
          "line": 315,
          "column": 18
        }
      },
      "secondFile": {
        "name": "tests/keras/utils/io_utils_test.py",
        "start": 289,
        "end": 299,
        "startLoc": {
          "line": 289,
          "column": 48
        },
        "endLoc": {
          "line": 299,
          "column": 13
        }
      }
    },
    {
      "format": "python",
      "lines": 11,
      "fragment": "):\n        K.clear_session()\n        with tf.device('/cpu:0'):\n            model = model_cls(weights=None,\n                              input_shape=(height, width, 3),\n                              classes=num_classes)\n        parallel_model = multi_gpu_model(model, gpus=i)\n        parallel_model.compile(loss='categorical_crossentropy',\n                               optimizer='rmsprop')\n\n        train_gen",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/utils/multi_gpu_test.py",
        "start": 244,
        "end": 254,
        "startLoc": {
          "line": 244,
          "column": 24
        },
        "endLoc": {
          "line": 254,
          "column": 18
        }
      },
      "secondFile": {
        "name": "tests/keras/utils/multi_gpu_test.py",
        "start": 175,
        "end": 185,
        "startLoc": {
          "line": 175,
          "column": 27
        },
        "endLoc": {
          "line": 185,
          "column": 19
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": ", batch_size=batch_size)\n    assert np.isscalar(score) and np.isfinite(score)\n\n    preds = clf.predict(X_test, batch_size=batch_size)\n    assert preds.shape == (num_test, )\n    for prediction in np.unique(preds):\n        assert prediction in string_classes",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/wrappers/scikit_learn_test.py",
        "start": 100,
        "end": 106,
        "startLoc": {
          "line": 100,
          "column": 43
        },
        "endLoc": {
          "line": 106,
          "column": 44
        }
      },
      "secondFile": {
        "name": "tests/keras/wrappers/scikit_learn_test.py",
        "start": 81,
        "end": 87,
        "startLoc": {
          "line": 81,
          "column": 39
        },
        "endLoc": {
          "line": 87,
          "column": 35
        }
      }
    },
    {
      "format": "python",
      "lines": 7,
      "fragment": "):\n    model = Sequential()\n    model.add(Dense(input_dim, input_shape=(input_dim,)))\n    model.add(Activation('relu'))\n    model.add(Dense(hidden_dims))\n    model.add(Activation('relu'))\n    model.add(Dense(1",
      "tokens": 0,
      "firstFile": {
        "name": "tests/keras/wrappers/scikit_learn_test.py",
        "start": 113,
        "end": 119,
        "startLoc": {
          "line": 113,
          "column": 32
        },
        "endLoc": {
          "line": 119,
          "column": 22
        }
      },
      "secondFile": {
        "name": "tests/keras/wrappers/scikit_learn_test.py",
        "start": 27,
        "end": 33,
        "startLoc": {
          "line": 27,
          "column": 29
        },
        "endLoc": {
          "line": 33,
          "column": 32
        }
      }
    }
  ],
  "statistics": {
    "detectionDate": "2020-02-13T12:21:00.233Z",
    "formats": {
      "yaml": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/.travis.yml": {
            "lines": 102,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/stale.yml": {
            "lines": 19,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/mkdocs.yml": {
            "lines": 90,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 211,
          "sources": 3,
          "clones": 0,
          "duplicatedLines": 0,
          "percentage": 0,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "markdown": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/CONTRIBUTING.md": {
            "lines": 95,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/ISSUE_TEMPLATE.md": {
            "lines": 13,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/PULL_REQUEST_TEMPLATE.md": {
            "lines": 18,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/README.md": {
            "lines": 201,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docker/README.md": {
            "lines": 56,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/README.md": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/README.md": {
            "lines": 137,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/ISSUE_TEMPLATE/a--tensorflow-backend-users.md": {
            "lines": 33,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/ISSUE_TEMPLATE/b--theano-backend-users.md": {
            "lines": 18,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 12,
            "percentage": 66.67,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/ISSUE_TEMPLATE/c--cntk-backend-users.md": {
            "lines": 17,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 12,
            "percentage": 70.59,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/ISSUE_TEMPLATE/d--keras-preprocessing-users.md": {
            "lines": 12,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/ISSUE_TEMPLATE/e--keras-applications-users.md": {
            "lines": 12,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/PULL_REQUEST_TEMPLATE/a--bug-fix.md": {
            "lines": 19,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/.github/PULL_REQUEST_TEMPLATE/b--new-feature.md": {
            "lines": 28,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/activations.md": {
            "lines": 30,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/backend.md": {
            "lines": 139,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/callbacks.md": {
            "lines": 68,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/constraints.md": {
            "lines": 23,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/datasets.md": {
            "lines": 205,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 12,
            "percentage": 5.85,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/index.md": {
            "lines": 3,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/initializers.md": {
            "lines": 41,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/losses.md": {
            "lines": 39,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/metrics.md": {
            "lines": 55,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/optimizers.md": {
            "lines": 46,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/regularizers.md": {
            "lines": 44,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/scikit-learn-api.md": {
            "lines": 43,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/visualization.md": {
            "lines": 51,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/why-use-keras.md": {
            "lines": 78,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/getting-started/functional-api-guide.md": {
            "lines": 435,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/getting-started/sequential-model-guide.md": {
            "lines": 397,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/layers/about-keras-layers.md": {
            "lines": 35,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/layers/writing-your-own-keras-layers.md": {
            "lines": 66,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/models/about-keras-models.md": {
            "lines": 92,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/models/model.md": {
            "lines": 25,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/models/sequential.md": {
            "lines": 7,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/preprocessing/image.md": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/preprocessing/text.md": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 2594,
          "sources": 37,
          "clones": 2,
          "duplicatedLines": 18,
          "percentage": 0.69,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "ini": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/pytest.ini": {
            "lines": 24,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 24,
          "sources": 1,
          "clones": 0,
          "duplicatedLines": 0,
          "percentage": 0,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "python": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/setup.py": {
            "lines": 66,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/update_api.py": {
            "lines": 15,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/autogen.py": {
            "lines": 473,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/structure.py": {
            "lines": 358,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/addition_rnn.py": {
            "lines": 211,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/antirectifier.py": {
            "lines": 108,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 16,
            "percentage": 14.81,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/babi_memnn.py": {
            "lines": 231,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 39,
            "percentage": 16.88,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/babi_rnn.py": {
            "lines": 228,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 39,
            "percentage": 17.11,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/cifar10_cnn.py": {
            "lines": 131,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/cifar10_resnet.py": {
            "lines": 455,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 22,
            "percentage": 4.84,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/class_activation_maps.py": {
            "lines": 93,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/cnn_seq2seq.py": {
            "lines": 207,
            "sources": 1,
            "clones": 4,
            "duplicatedLines": 106,
            "percentage": 51.21,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/conv_filter_visualization.py": {
            "lines": 259,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/conv_lstm.py": {
            "lines": 143,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 24,
            "percentage": 16.78,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/deep_dream.py": {
            "lines": 192,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/imdb_bidirectional_lstm.py": {
            "lines": 49,
            "sources": 1,
            "clones": 3,
            "duplicatedLines": 40,
            "percentage": 81.63,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/imdb_cnn.py": {
            "lines": 73,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 28,
            "percentage": 38.36,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/imdb_cnn_lstm.py": {
            "lines": 76,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 14,
            "percentage": 18.42,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/imdb_fasttext.py": {
            "lines": 144,
            "sources": 1,
            "clones": 3,
            "duplicatedLines": 29,
            "percentage": 20.14,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/imdb_lstm.py": {
            "lines": 60,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 15,
            "percentage": 25,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/lstm_seq2seq.py": {
            "lines": 234,
            "sources": 1,
            "clones": 5,
            "duplicatedLines": 123,
            "percentage": 52.56,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/lstm_seq2seq_restore.py": {
            "lines": 151,
            "sources": 1,
            "clones": 3,
            "duplicatedLines": 101,
            "percentage": 66.89,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/lstm_stateful.py": {
            "lines": 243,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/lstm_text_generation.py": {
            "lines": 112,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_acgan.py": {
            "lines": 347,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_cnn.py": {
            "lines": 70,
            "sources": 1,
            "clones": 7,
            "duplicatedLines": 72,
            "percentage": 102.86,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_denoising_autoencoder.py": {
            "lines": 149,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 8,
            "percentage": 5.37,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_hierarchical_rnn.py": {
            "lines": 98,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 9,
            "percentage": 9.18,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_irnn.py": {
            "lines": 73,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 13,
            "percentage": 17.81,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_mlp.py": {
            "lines": 56,
            "sources": 1,
            "clones": 3,
            "duplicatedLines": 31,
            "percentage": 55.36,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_net2net.py": {
            "lines": 401,
            "sources": 1,
            "clones": 6,
            "duplicatedLines": 32,
            "percentage": 7.98,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_siamese.py": {
            "lines": 143,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_sklearn_wrapper.py": {
            "lines": 102,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 16,
            "percentage": 15.69,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_swwae.py": {
            "lines": 190,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 9,
            "percentage": 4.74,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/mnist_transfer_cnn.py": {
            "lines": 124,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 10,
            "percentage": 8.06,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/neural_doodle.py": {
            "lines": 382,
            "sources": 1,
            "clones": 5,
            "duplicatedLines": 74,
            "percentage": 19.37,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/neural_style_transfer.py": {
            "lines": 298,
            "sources": 1,
            "clones": 5,
            "duplicatedLines": 74,
            "percentage": 24.83,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/pretrained_word_embeddings.py": {
            "lines": 144,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/reuters_mlp.py": {
            "lines": 60,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 23,
            "percentage": 38.33,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/reuters_mlp_relu_vs_selu.py": {
            "lines": 175,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 23,
            "percentage": 13.14,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/variational_autoencoder.py": {
            "lines": 207,
            "sources": 1,
            "clones": 5,
            "duplicatedLines": 83,
            "percentage": 40.1,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/examples/variational_autoencoder_deconv.py": {
            "lines": 230,
            "sources": 1,
            "clones": 6,
            "duplicatedLines": 91,
            "percentage": 39.57,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/__init__.py": {
            "lines": 26,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/activations.py": {
            "lines": 239,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/constraints.py": {
            "lines": 183,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 14,
            "percentage": 7.65,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/models.py": {
            "lines": 257,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/objectives.py": {
            "lines": 6,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/regularizers.py": {
            "lines": 88,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 14,
            "percentage": 15.91,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/conftest.py": {
            "lines": 13,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/test_api.py": {
            "lines": 32,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/test_dynamic_trainability.py": {
            "lines": 110,
            "sources": 1,
            "clones": 4,
            "duplicatedLines": 44,
            "percentage": 40,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/test_loss_masking.py": {
            "lines": 55,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/test_loss_weighting.py": {
            "lines": 174,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 18,
            "percentage": 10.34,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/test_model_pickling.py": {
            "lines": 147,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 16,
            "percentage": 10.88,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/__init__.py": {
            "lines": 36,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/densenet.py": {
            "lines": 31,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/imagenet_utils.py": {
            "lines": 19,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/inception_resnet_v2.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/inception_v3.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/mobilenet.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/mobilenet_v2.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/nasnet.py": {
            "lines": 26,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet.py": {
            "lines": 34,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet50.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet_v2.py": {
            "lines": 34,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/vgg16.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/vgg19.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/xception.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/backend/__init__.py": {
            "lines": 164,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/backend/common.py": {
            "lines": 232,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/backend/load_backend.py": {
            "lines": 124,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/callbacks/__init__.py": {
            "lines": 22,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/callbacks/tensorboard_v1.py": {
            "lines": 362,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/callbacks/tensorboard_v2.py": {
            "lines": 116,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/__init__.py": {
            "lines": 9,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/boston_housing.py": {
            "lines": 43,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/cifar.py": {
            "lines": 37,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/cifar10.py": {
            "lines": 44,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 9,
            "percentage": 20.45,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/cifar100.py": {
            "lines": 46,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 9,
            "percentage": 19.57,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/fashion_mnist.py": {
            "lines": 45,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/imdb.py": {
            "lines": 121,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 15,
            "percentage": 12.4,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/mnist.py": {
            "lines": 27,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/datasets/reuters.py": {
            "lines": 105,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 15,
            "percentage": 14.29,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/__init__.py": {
            "lines": 8,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/input_layer.py": {
            "lines": 182,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/sequential.py": {
            "lines": 305,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/topology.py": {
            "lines": 5,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/training_arrays.py": {
            "lines": 471,
            "sources": 1,
            "clones": 4,
            "duplicatedLines": 38,
            "percentage": 8.07,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/__init__.py": {
            "lines": 168,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/advanced_activations.py": {
            "lines": 321,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/embeddings.py": {
            "lines": 161,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/local.py": {
            "lines": 393,
            "sources": 1,
            "clones": 8,
            "duplicatedLines": 88,
            "percentage": 22.39,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/noise.py": {
            "lines": 176,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/normalization.py": {
            "lines": 232,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/preprocessing/__init__.py": {
            "lines": 12,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/preprocessing/sequence.py": {
            "lines": 78,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/preprocessing/text.py": {
            "lines": 13,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/__init__.py": {
            "lines": 30,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/conv_utils.py": {
            "lines": 181,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/io_utils.py": {
            "lines": 430,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/layer_utils.py": {
            "lines": 300,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/losses_utils.py": {
            "lines": 178,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/metrics_utils.py": {
            "lines": 324,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/multi_gpu_utils.py": {
            "lines": 258,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/np_utils.py": {
            "lines": 71,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/test_utils.py": {
            "lines": 310,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/vis_utils.py": {
            "lines": 253,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/wrappers/scikit_learn.py": {
            "lines": 347,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/docs/test_doc_auto_generation.py": {
            "lines": 422,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/docs/test_documentation.py": {
            "lines": 180,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/applications_test.py": {
            "lines": 78,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/imagenet_utils_test.py": {
            "lines": 113,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 14,
            "percentage": 12.39,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_datasets.py": {
            "lines": 90,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_image_data_tasks.py": {
            "lines": 80,
            "sources": 1,
            "clones": 3,
            "duplicatedLines": 53,
            "percentage": 66.25,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_temporal_data_tasks.py": {
            "lines": 180,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 32,
            "percentage": 17.78,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_tensorflow_integration.py": {
            "lines": 55,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_vector_data_tasks.py": {
            "lines": 90,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/activations_test.py": {
            "lines": 248,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 10,
            "percentage": 4.03,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/constraints_test.py": {
            "lines": 79,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/initializers_test.py": {
            "lines": 166,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/metrics_correctness_test.py": {
            "lines": 379,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 14,
            "percentage": 3.69,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/metrics_functional_test.py": {
            "lines": 125,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/metrics_training_test.py": {
            "lines": 94,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/optimizers_test.py": {
            "lines": 179,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 14,
            "percentage": 7.82,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/regularizers_test.py": {
            "lines": 109,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/test_sequential_model.py": {
            "lines": 497,
            "sources": 1,
            "clones": 11,
            "duplicatedLines": 115,
            "percentage": 23.14,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/preprocessing/sequence_test.py": {
            "lines": 242,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/preprocessing/text_test.py": {
            "lines": 126,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/callbacks/tensorboard_test.py": {
            "lines": 273,
            "sources": 1,
            "clones": 1,
            "duplicatedLines": 9,
            "percentage": 3.3,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/datasets/datasets_test.py": {
            "lines": 90,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/engine/layer_subclassing_tests.py": {
            "lines": 203,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/advanced_activations_test.py": {
            "lines": 78,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/convolutional_recurrent_test.py": {
            "lines": 168,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/core_test.py": {
            "lines": 370,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/cudnn_recurrent_test.py": {
            "lines": 384,
            "sources": 1,
            "clones": 6,
            "duplicatedLines": 96,
            "percentage": 25,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/embeddings_test.py": {
            "lines": 47,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/local_test.py": {
            "lines": 60,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/merge_test.py": {
            "lines": 292,
            "sources": 1,
            "clones": 14,
            "duplicatedLines": 104,
            "percentage": 35.62,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/noise_test.py": {
            "lines": 32,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/normalization_test.py": {
            "lines": 244,
            "sources": 1,
            "clones": 5,
            "duplicatedLines": 53,
            "percentage": 21.72,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/pooling_test.py": {
            "lines": 153,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/legacy/conftest.py": {
            "lines": 12,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/legacy/layers_test.py": {
            "lines": 39,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 16,
            "percentage": 41.03,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/conv_utils_test.py": {
            "lines": 82,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/generic_utils_test.py": {
            "lines": 137,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/io_utils_test.py": {
            "lines": 352,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 20,
            "percentage": 5.68,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/layer_utils_test.py": {
            "lines": 64,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/multi_gpu_test.py": {
            "lines": 304,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 20,
            "percentage": 6.58,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/np_utils_test.py": {
            "lines": 33,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/vis_utils_test.py": {
            "lines": 56,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/wrappers/scikit_learn_test.py": {
            "lines": 204,
            "sources": 1,
            "clones": 4,
            "duplicatedLines": 24,
            "percentage": 11.76,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 23152,
          "sources": 151,
          "clones": 82,
          "duplicatedLines": 1019,
          "percentage": 4.4,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "bash": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/.travis/install_cntk.sh": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 11,
          "sources": 1,
          "clones": 0,
          "duplicatedLines": 0,
          "percentage": 0,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "unknown": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/keras/regularizers.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/conftest.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/densenet.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/imagenet_utils.py": {
            "lines": 7,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/inception_resnet_v2.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/inception_v3.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/mobilenet.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/mobilenet_v2.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/nasnet.py": {
            "lines": 16,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet50.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/resnet_v2.py": {
            "lines": 21,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/vgg16.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/vgg19.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/applications/xception.py": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/input_layer.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/engine/sequential.py": {
            "lines": 194,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/advanced_activations.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/embeddings.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/local.py": {
            "lines": 185,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/noise.py": {
            "lines": 46,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/layers/normalization.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/io_utils.py": {
            "lines": 93,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/losses_utils.py": {
            "lines": 5,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/metrics_utils.py": {
            "lines": 29,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/keras/utils/test_utils.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/docs/test_doc_auto_generation.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/docs/test_documentation.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_temporal_data_tasks.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/integration_tests/test_tensorflow_integration.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/initializers_test.py": {
            "lines": 117,
            "sources": 1,
            "clones": 2,
            "duplicatedLines": 220,
            "percentage": 188.03,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/metrics_functional_test.py": {
            "lines": 70,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/metrics_training_test.py": {
            "lines": 34,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/optimizers_test.py": {
            "lines": 72,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/test_sequential_model.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/callbacks/tensorboard_test.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/datasets/datasets_test.py": {
            "lines": 35,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/engine/layer_subclassing_tests.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/advanced_activations_test.py": {
            "lines": 56,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/convolutional_recurrent_test.py": {
            "lines": 3,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/core_test.py": {
            "lines": 212,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/cudnn_recurrent_test.py": {
            "lines": 315,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/embeddings_test.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/noise_test.py": {
            "lines": 17,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/normalization_test.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/layers/pooling_test.py": {
            "lines": 129,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/legacy/conftest.py": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/generic_utils_test.py": {
            "lines": 80,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/tests/keras/utils/io_utils_test.py": {
            "lines": 212,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 2095,
          "sources": 49,
          "clones": 1,
          "duplicatedLines": 110,
          "percentage": 5.25,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "css": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/docs/templates/index.md": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/why-use-keras.md": {
            "lines": 63,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/versions.html": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/getting-started/functional-api-guide.md": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/templates/getting-started/sequential-model-guide.md": {
            "lines": 1,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/css/theme_extra.css": {
            "lines": 146,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 223,
          "sources": 6,
          "clones": 0,
          "duplicatedLines": 0,
          "percentage": 0,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      },
      "markup": {
        "sources": {
          "/Users/ms/dev/hackathon/keras/docs/theme/404.html": {
            "lines": 9,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/base.html": {
            "lines": 168,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/breadcrumbs.html": {
            "lines": 41,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/footer.html": {
            "lines": 26,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/nav.html": {
            "lines": 22,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/search.html": {
            "lines": 16,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/searchbox.html": {
            "lines": 5,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/toc.html": {
            "lines": 11,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          },
          "/Users/ms/dev/hackathon/keras/docs/theme/versions.html": {
            "lines": 17,
            "sources": 1,
            "clones": 0,
            "duplicatedLines": 0,
            "percentage": 0,
            "newDuplicatedLines": 0,
            "newClones": 0
          }
        },
        "total": {
          "lines": 315,
          "sources": 9,
          "clones": 0,
          "duplicatedLines": 0,
          "percentage": 0,
          "newDuplicatedLines": 0,
          "newClones": 0
        }
      }
    },
    "total": {
      "lines": 28625,
      "sources": 257,
      "clones": 85,
      "duplicatedLines": 1147,
      "percentage": 4.01,
      "newDuplicatedLines": 0,
      "newClones": 0
    }
  }
}